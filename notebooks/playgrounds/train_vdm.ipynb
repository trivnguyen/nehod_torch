{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import matplotlib.pyplot as plt\n",
    "import ml_collections\n",
    "from ml_collections import config_dict\n",
    "\n",
    "from models import noise_schedules\n",
    "from models import transformer, scores, vdm\n",
    "from models import diffusion_utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "\n",
    "    config = config_dict.ConfigDict()\n",
    "\n",
    "    config.workdir = 'trained_models/test2'\n",
    "\n",
    "    config.data = data = config_dict.ConfigDict()\n",
    "    data.conditioning_parameters = [\n",
    "        \"halo_mvir\", \"halo_mstar\", \"center_subhalo_mvir\", \"center_subhalo_mstar\",\n",
    "        \"center_subhalo_vpeculiar\", \"center_subhalo_vmax_tilde\", \"log_num_subhalos\",\n",
    "        \"inv_wdm_mass\", \"log_sn1\", \"log_sn2\", \"log_agn1\"\n",
    "    ]\n",
    "\n",
    "    # VDM args\n",
    "    config.vdm = vdm = config_dict.ConfigDict()\n",
    "    vdm.d_in = 9\n",
    "    vdm.d_cond = len(data.conditioning_parameters)\n",
    "    vdm.d_context_embedding = 16\n",
    "    vdm.timesteps = 0\n",
    "    vdm.antithetic_time_sampling = True\n",
    "    vdm.use_encdec = False\n",
    "    vdm.embed_context = True\n",
    "    \n",
    "    vdm.score_model = score = config_dict.ConfigDict()\n",
    "    score.name = 'transformer'\n",
    "    score.d_t_embedding = 16\n",
    "    score.d_model = 128\n",
    "    score.d_mlp = 256\n",
    "    score.d_cond = vdm.d_context_embedding if vdm.embed_context else vdm.d_cond\n",
    "    score.n_layers = 6\n",
    "    score.n_heads = 4\n",
    "\n",
    "    vdm.noise_schedule = noise_schedule = config_dict.ConfigDict()\n",
    "    noise_schedule.name = \"learned_linear\"\n",
    "    noise_schedule.gamma_min = -16.0\n",
    "    noise_schedule.gamma_max = 10.0\n",
    "\n",
    "    # training and loss args\n",
    "    config.training = training = config_dict.ConfigDict()\n",
    "    training.batch_size = 128\n",
    "    training.max_steps = 50_000\n",
    "    training.noise_scale = 1e-3\n",
    "    training.beta = 1.0\n",
    "    training.rotation_augmentation = True\n",
    "    training.n_pos_dim = 3\n",
    "    training.n_vel_dim = 3\n",
    "    training.add_mass_recon_loss = True\n",
    "    training.i_mass_start = 6\n",
    "    training.i_mass_stop = 8\n",
    "\n",
    "    # optimizer and scheduler args\n",
    "    config.optimizer = optimizer = config_dict.ConfigDict()\n",
    "    optimizer.name = \"AdamW\"\n",
    "    optimizer.lr = 5e-4\n",
    "    optimizer.betas = [0.9, 0.999]\n",
    "    optimizer.weight_decay = 0.01\n",
    "    optimizer.grad_clip = 0.5\n",
    "    config.scheduler = scheduler = config_dict.ConfigDict()\n",
    "    scheduler.name = \"WarmUpCosineDecayLR\"\n",
    "    scheduler.init_value = 0.0\n",
    "    scheduler.peak_value = optimizer.lr\n",
    "    scheduler.warmup_steps = 5_000\n",
    "    scheduler.decay_steps = training.max_steps\n",
    "\n",
    "    return config\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "datadir = '/mnt/ceph/users/tnguyen/dark_camels/point-cloud-diffusion-datasets/processed_datasets/final-WDM-datasets'\n",
    "\n",
    "train_data = np.load(os.path.join(datadir, 'train_galprop.npz'))\n",
    "train_cond_table = pd.read_csv(os.path.join(datadir, 'train_galprop_cond.csv'))\n",
    "val_data = np.load(os.path.join(datadir, 'val_galprop.npz'))\n",
    "val_cond_table = pd.read_csv(os.path.join(datadir, 'val_galprop_cond.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-6\n",
    "\n",
    "# preprocess the data\n",
    "train_x = train_data['features']\n",
    "train_mask = train_data['mask']\n",
    "train_cond = train_cond_table[config.data.conditioning_parameters].values\n",
    "val_x = val_data['features']\n",
    "val_mask = val_data['mask']\n",
    "val_cond = val_cond_table[config.data.conditioning_parameters].values\n",
    "\n",
    "# normalize the data and convert to torch tensors\n",
    "mask_bool = train_mask.astype(bool)\n",
    "x_mean = np.mean(train_x, axis=(0, 1), where=mask_bool[..., None])\n",
    "x_std = np.std(train_x, axis=(0, 1), where=mask_bool[..., None])\n",
    "cond_mean = np.mean(train_cond, axis=0)\n",
    "cond_std = np.std(train_cond, axis=0)\n",
    "norm_dict = {\n",
    "    'x_mean': x_mean,\n",
    "    'x_std': x_std,\n",
    "    'cond_mean': cond_mean,\n",
    "    'cond_std': cond_std\n",
    "}\n",
    "\n",
    "train_x = (train_x - x_mean + EPS) / (x_std + EPS)\n",
    "train_cond = (train_cond - cond_mean + EPS) / (cond_std + EPS)\n",
    "val_x = (val_x - x_mean + EPS) / (x_std + EPS)\n",
    "val_cond = (val_cond - cond_mean + EPS) / (cond_std + EPS)\n",
    "\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "train_cond = torch.tensor(train_cond, dtype=torch.float32)\n",
    "val_x = torch.tensor(val_x, dtype=torch.float32)\n",
    "val_cond = torch.tensor(val_cond, dtype=torch.float32)\n",
    "# invert mask due to torch convention vs jax\n",
    "train_mask = ~torch.tensor(train_mask, dtype=torch.bool)\n",
    "val_mask = ~torch.tensor(val_mask, dtype=torch.bool)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dset = TensorDataset(train_x, train_cond, train_mask)\n",
    "val_dset = TensorDataset(val_x, val_cond, val_mask)\n",
    "train_loader = DataLoader(\n",
    "    train_dset, batch_size=config.training.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(\n",
    "    val_dset, batch_size=config.training.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/tnguyen/miniconda3/envs/geometric/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/tnguyen/miniconda3/envs/geometric/lib/pyth ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/mnt/home/tnguyen/miniconda3/envs/geometric/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /mnt/home/tnguyen/miniconda3/envs/geometric/lib/pyth ...\n",
      "Missing logger folder: trained_models/test2/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                  | Params\n",
      "------------------------------------------------------------\n",
      "0 | gamma             | NoiseScheduleScalar   | 2     \n",
      "1 | score_model       | TransformerScoreModel | 845 K \n",
      "2 | embedding_context | Linear                | 192   \n",
      "------------------------------------------------------------\n",
      "846 K     Trainable params\n",
      "0         Non-trainable params\n",
      "846 K     Total params\n",
      "3.384     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/tnguyen/miniconda3/envs/geometric/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/tnguyen/miniconda3/envs/geometric/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/mnt/home/tnguyen/miniconda3/envs/geometric/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 7/7 [00:00<00:00, 18.62it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 225.326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 7/7 [00:00<00:00, 17.12it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 17.676 >= min_delta = 0.0. New best score: 207.650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 7/7 [00:00<00:00, 18.32it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.514 >= min_delta = 0.0. New best score: 206.136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 7/7 [00:00<00:00, 17.34it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 2.299 >= min_delta = 0.0. New best score: 203.837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 7/7 [00:00<00:00, 16.54it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 6.450 >= min_delta = 0.0. New best score: 197.387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 7/7 [00:00<00:00, 16.66it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 2.892 >= min_delta = 0.0. New best score: 194.495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 7/7 [00:00<00:00, 16.43it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.255 >= min_delta = 0.0. New best score: 193.240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 7/7 [00:00<00:00, 17.48it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 4.927 >= min_delta = 0.0. New best score: 188.313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 7/7 [00:00<00:00, 17.55it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 2.576 >= min_delta = 0.0. New best score: 185.738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 7/7 [00:00<00:00, 17.63it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 3.102 >= min_delta = 0.0. New best score: 182.635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 7/7 [00:00<00:00, 17.25it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 4.612 >= min_delta = 0.0. New best score: 178.024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 7/7 [00:00<00:00, 18.76it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 6.008 >= min_delta = 0.0. New best score: 172.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 7/7 [00:00<00:00, 19.34it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.638 >= min_delta = 0.0. New best score: 171.378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 7/7 [00:00<00:00, 18.37it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 1.157 >= min_delta = 0.0. New best score: 170.221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 7/7 [00:00<00:00, 18.61it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 7.339 >= min_delta = 0.0. New best score: 162.882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 7/7 [00:00<00:00, 19.88it/s, v_num=0]"
     ]
    }
   ],
   "source": [
    "model = vdm.VariationalDiffusionModel(\n",
    "    d_in=config.vdm.d_in,\n",
    "    d_cond=config.vdm.d_cond,\n",
    "    d_context_embedding=config.vdm.d_context_embedding,\n",
    "    embed_context=config.vdm.embed_context,\n",
    "    score_model_args=config.vdm.score_model,\n",
    "    noise_schedule_args=config.vdm.noise_schedule,\n",
    "    timesteps=config.vdm.timesteps,\n",
    "    antithetic_time_sampling=config.vdm.antithetic_time_sampling,\n",
    "    use_encdec=config.vdm.use_encdec,\n",
    "    training_args=config.training,\n",
    "    optimizer_args=config.optimizer,\n",
    "    scheduler_args=config.scheduler,\n",
    "    norm_dict=norm_dict\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    pl.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=1000, mode='min', verbose=True),\n",
    "    pl.callbacks.ModelCheckpoint(\n",
    "        monitor='val_loss', save_top_k=10, mode='min', save_weights_only=False),\n",
    "    pl.callbacks.LearningRateMonitor(\"step\"),\n",
    "]\n",
    "train_logger = pl_loggers.TensorBoardLogger(config.workdir)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=config.workdir,\n",
    "    max_steps=config.training.max_steps,\n",
    "    accelerator='gpu',\n",
    "    callbacks=callbacks,\n",
    "    logger=train_logger,\n",
    "    enable_progress_bar=True,\n",
    "    inference_mode=False,\n",
    "    gradient_clip_val=config.optimizer.grad_clip,\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
